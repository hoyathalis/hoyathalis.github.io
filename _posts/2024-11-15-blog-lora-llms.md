---
title: 'ML4LM â€” Fine-Tune Smarter, Not Harder: Discover LoRA for LLMs'
date: 2024-11-15
permalink: /posts/2024/11/LoRA-LLMs/
medium_link: "https://hoyath.medium.com/ml4lm-fine-tune-smarter-not-harder-discover-lora-for-llms-24c59853a0f2"
tags:
  - Machine Learning
  - Large Language Models
  - LoRA
  - Fine-tuning
---

# Fine-Tune Smarter, Not Harder: Discover LoRA for LLMs

When fine-tuning a Large Language Model (LLM), instead of adjusting all the original weights, we can train a smaller set of new weights...

Original Weights of LLM are freezed /unchanged, only LORA weights A, B matrices are trained and are multiplied and added on to original weights by taking a ratio

Introduction to Low-Rank Adaptation (LoRA) for efficient fine-tuning of Large Language Models, reducing computational requirements while maintaining performance.

[Read the full article on Medium](https://hoyath.medium.com/ml4lm-fine-tune-smarter-not-harder-discover-lora-for-llms-24c59853a0f2)
